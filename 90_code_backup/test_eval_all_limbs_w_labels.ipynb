{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from super_gradients.training import models\n",
    "from super_gradients.training import Trainer, models\n",
    "from super_gradients.training import dataloaders\n",
    "from super_gradients.training.dataloaders.dataloaders import (\n",
    "    coco_detection_yolo_format_train, \n",
    "    coco_detection_yolo_format_val,\n",
    ")\n",
    "import super_gradients.training\n",
    "super_gradients.setup_device(device='cuda')\n",
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import (\n",
    "    DetectionMetrics_050,\n",
    "    DetectionMetrics,\n",
    ")\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = 'cuda:0'\n",
    "BATCH_SIZE = 1\n",
    "WORKERS = 8\n",
    "#model = torch.load('/home/matthias/workspace/Coding/00_vista_medizina/bone_frac_obj_det/yoloNAS/checkpoints/yolo_nas_s/RUN_20241117_212610_007684/ckpt_epoch_100.pth', map_location=torch.device(DEVICE))\n",
    "\n",
    "# %%\n",
    "ROOT_DIR = '/home/matthias/workspace/Coding/00_vista_medizina/00_data/2024-11-21/single_class_all_categories'\n",
    "train_imgs_dir = ROOT_DIR + '/train/images'\n",
    "train_labels_dir = ROOT_DIR + '/train/labels'\n",
    "val_imgs_dir = ROOT_DIR + '/val/images'\n",
    "val_labels_dir = ROOT_DIR + '/val/labels'\n",
    "test_imgs_dir = ROOT_DIR + '/test/images'\n",
    "test_labels_dir = ROOT_DIR + '/test/labels'\n",
    "classes = ['fracture']\n",
    "\n",
    "# %%\n",
    "dataset_params = {\n",
    "    'data_dir': ROOT_DIR,\n",
    "    'train_images_dir': train_imgs_dir,\n",
    "    'train_labels_dir': train_labels_dir,\n",
    "    'val_images_dir': val_imgs_dir,\n",
    "    'val_labels_dir': val_labels_dir,\n",
    "    'test_images_dir': test_imgs_dir,\n",
    "    'test_labels_dir': test_labels_dir,\n",
    "    'classes': classes,\n",
    "    'ignore_empty_annotations': True,\n",
    "}\n",
    "\n",
    "test_data = coco_detection_yolo_format_val(\n",
    "    dataset_params={\n",
    "        'data_dir': dataset_params['data_dir'],\n",
    "        'images_dir': dataset_params['test_images_dir'],\n",
    "        'labels_dir': dataset_params['test_labels_dir'],\n",
    "        'classes': dataset_params['classes'],\n",
    "        'ignore_empty_annotations': dataset_params['ignore_empty_annotations'],\n",
    "    },\n",
    "    dataloader_params={\n",
    "        'batch_size':BATCH_SIZE,\n",
    "        'num_workers':WORKERS,\n",
    "    }\n",
    ")\n",
    "\n",
    "dict_model_paths = {\n",
    "    'f1@0.5opt': \"/home/matthias/workspace/Coding/00_vista_medizina/vista_bone_frac/yoloNAS/checkpoints/yolo_nas_s/RUN_20241123_101156_209514/ckpt_best.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_loaded = {}\n",
    "for model_name in list(dict_model_paths.keys()):\n",
    "    print(\"Model: \", model_name)\n",
    "    dict_models_loaded.update({model_name: \n",
    "                        models.get('yolo_nas_s',\n",
    "                        num_classes=80,\n",
    "                        checkpoint_path=dict_model_paths[model_name])})\n",
    "print(\"Models: \", dict_models_loaded.keys())\n",
    "\n",
    "\n",
    "def load_ground_truth(gt_txt_path) -> list[list[float | int]]:\n",
    "    ground_truths = []\n",
    "    with open(gt_txt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            # Convert from normalized to pixel coordinates\n",
    "            x_min = float(parts[1]) #* img_width\n",
    "            y_min = float(parts[2]) #* img_height\n",
    "            x_max = float(parts[3]) #* img_width\n",
    "            y_max = float(parts[4]) #* img_height\n",
    "            \n",
    "            # Calculate the bounding box corners (x_min, y_min, x_max, y_max)\n",
    "            #x_min = int(x_center - width / 2)\n",
    "            #y_min = int(y_center - height / 2)\n",
    "            #x_max = int(x_center + width / 2)\n",
    "            #y_max = int(y_center + height / 2)\n",
    "            \n",
    "            ground_truths.append([x_min, y_min, x_max, y_max, class_id])\n",
    "\n",
    "    return ground_truths\n",
    "\n",
    "\n",
    "def print_boxes_to_image(image_data: np.ndarray, boxes_info, ground_truth: bool):\n",
    "\n",
    "    height, width, channels = image_data.shape\n",
    "    \n",
    "    if ground_truth:\n",
    "        if len(boxes_info) == 0: \n",
    "            cv2.putText(image_data, 'No fractures in ground truth data.', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        else:\n",
    "            for box in boxes_info:\n",
    "                x_center_frac, y_center_frac, width_frac, height_frac, class_id = box\n",
    "\n",
    "                x_center = x_center_frac * width\n",
    "                y_center = y_center_frac * height\n",
    "                width = width_frac * width\n",
    "                height = height_frac * height\n",
    "\n",
    "                # Calculate top-left and bottom-right corners\n",
    "                x1 = int(x_center - width / 2)\n",
    "                y1 = int(y_center - height / 2)\n",
    "                x2 = int(x_center + width / 2)\n",
    "                y2 = int(y_center + height / 2)\n",
    "\n",
    "                box_text = f'GT Class {class_id}'\n",
    "                \n",
    "                cv2.rectangle(image_data, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(image_data, box_text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    else:\n",
    "        for idx, box in enumerate(boxes_info.prediction.bboxes_xyxy):\n",
    "            x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
    "\n",
    "            class_id = boxes_info.prediction.labels[idx]\n",
    "            score = boxes_info.prediction.confidence[idx]\n",
    "            box_text = f'Pred Class {class_id}: {score:.2f}'  # Use actual label or class mapping here\n",
    "            \n",
    "            cv2.rectangle(image_data, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(image_data, box_text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "def get_label_and_image_paths(path_images_dir: str, path_labels_dir: str) -> tuple[list]:\n",
    "    list_paths_images = []\n",
    "    for dirpath, _, filenames in os.walk(path_images_dir):\n",
    "        \n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.' + 'jpg'):\n",
    "                list_paths_images.append(os.path.join(dirpath, filename))\n",
    "    list_paths_images\n",
    "\n",
    "    #list_paths_labels = []\n",
    "    #for f in list_paths_images:\n",
    "    #    list_paths_labels.append(\".\".join(f.split('.')[:-1]) + \".txt\")\n",
    "\n",
    "    list_paths_labels = []\n",
    "    for dirpath, _, filenames in os.walk(path_labels_dir):\n",
    "        \n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.' + 'txt'):\n",
    "                list_paths_labels.append(os.path.join(dirpath, filename))\n",
    "    list_paths_labels\n",
    "\n",
    "    return {\n",
    "        'list_paths_images': list_paths_images, \n",
    "        'list_paths_labels': list_paths_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "dict_paths_labels_images = get_label_and_image_paths(\n",
    "    path_images_dir=dataset_params['test_images_dir'], \n",
    "    path_labels_dir=dataset_params['test_labels_dir'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path_image in dict_paths_labels_images['list_paths_images']:\n",
    "\n",
    "    image = cv2.imread(path_image)\n",
    "\n",
    "    trainer = Trainer(\n",
    "            experiment_name='yolo_nas_s',\n",
    "        )\n",
    "    predictions = dict_models_loaded[list(dict_models_loaded.keys())[0]].predict(\n",
    "        images=path_image,\n",
    "        iou=0.0,\n",
    "        conf=0.2,\n",
    "        max_predictions=2,\n",
    "    )\n",
    "    #predictions = trainer.predict(path_image)\n",
    "\n",
    "\n",
    "    path_gt_data = \".\".join(path_image.split('/')[-1].split('.')[:-1]) + \".txt\"\n",
    "    path_gt_data = test_labels_dir + '/' + path_gt_data\n",
    "    #path_gt_data = \".\".join(path_image.split('.')[:-1]) + \".txt\"\n",
    "    if Path(path_gt_data).exists():\n",
    "        ground_truths = load_ground_truth(path_gt_data)\n",
    "        print_boxes_to_image(image_data=image, boxes_info=ground_truths, ground_truth=True)\n",
    "    \n",
    "    print_boxes_to_image(image_data=image, boxes_info=predictions, ground_truth=False)\n",
    "\n",
    "    cv2.imshow('', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_yoloNAS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import *\n",
    "from visualisation.core.utils import device\n",
    "\n",
    "model = alexnet(pretrained=True).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some inputs images. We are going to use three pictures, a cat, the beautiful Basilica di San Pietro and an image with a dog and a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded a few packages. In `utils` there are several utility function to creates the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from visualisation.core.utils import device\n",
    "from PIL import Image\n",
    "\n",
    "image_paths = glob.glob(\"./images/*.*\")\n",
    "\n",
    "images = list(map(lambda x: Image.open(x), image_paths))\n",
    "\n",
    "subplot(images, title=\"inputs\", rows_titles=[\"cat\", \"san pietro\", \"dog_cat\"], nrows=1, ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Resize, Compose, ToPILImage\n",
    "from visualisation.core import *\n",
    "from visualisation.core.utils import image_net_preprocessing\n",
    "\n",
    "inputs = [\n",
    "    Compose([Resize((224, 224)), ToTensor(), image_net_preprocessing])(x).unsqueeze(0)\n",
    "    for x in images\n",
    "]  # add 1 dim for batch\n",
    "inputs = [i.to(device) for i in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an utility function to clean the gpu cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free(list_models: list):\n",
    "    for m in list_models:\n",
    "        del m\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Weights(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_layer = model_traced[0]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 16\n",
    "\n",
    "run_vis_plot(vis, inputs[0], first_layer, ncols=4, nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_maxpool_layer = model_traced[2]\n",
    "run_vis_plot(vis, inputs[0], first_maxpool_layer, ncols=4, nrows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with an other input, the San Pietro Basilica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_vis_plot(vis, inputs[1], first_maxpool_layer, ncols=4, nrows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at them, these images make somehow sense; they highlight the basilica layout but it is hard to understand what the model is actually doing. We got the idea that is computing something correctly but we could ask some questions, for example: is it looking at the cupola? Which are the most important features of the Basilica?\n",
    "\n",
    "Moreover, the deeper we go the harder it becomes to even recognize the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeper_layer = model_traced[6]\n",
    "run_vis_plot(vis, inputs[1], deeper_layer, ncols=4, nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alexnet(pretrained=True).to(device)\n",
    "\n",
    "run_vis_plot_across_models(modules, inputs[0], None, GradCam , 'Gradcam', device,\n",
    "                           nrows=1, \n",
    "                           ncols=4, \n",
    "                           target_class=None, \n",
    "                           postprocessing=image_net_postprocessing)\n",
    "free([alexnet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visualisation.core.utils import imshow\n",
    "\n",
    "module = module.to(device)\n",
    "\n",
    "vis = GradCam(module, device)\n",
    "\n",
    "classes = [None, 285, 453]\n",
    "outs = [\n",
    "    vis(inputs[0], None, postprocessing=image_net_postprocessing, target_class=c) for c in classes\n",
    "]\n",
    "\n",
    "images, classes = vis_outs2images_classes(outs)\n",
    "\n",
    "subplot(images, title=\"resnet34\", rows_titles=classes, nrows=1, ncols=len(outs), parse=tensor2img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how similar to the `CAM` output they are. To better compore our three models, below we plot the grad cam for each input with respect to each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modules = (\n",
    "    m(pretrained=True).to(device) for m in modules_instances\n",
    ")  # make a generator, we don't want to store in memory all of them at once\n",
    "\n",
    "run_vis_plot_across_models(\n",
    "    modules,\n",
    "    inputs[0],\n",
    "    None,\n",
    "    GradCam,\n",
    "    \"Gradcam\",\n",
    "    device,\n",
    "    nrows=4,\n",
    "    ncols=3,\n",
    "    target_class=None,\n",
    "    inputs=inputs,\n",
    "    idx2label=imagenet2human,\n",
    "    annotations=[\"alexnet\", \"vgg16\", \"resnet34\", \"resnet152\"],\n",
    "    postprocessing=image_net_postprocessing,\n",
    ")\n",
    "\n",
    "free(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader can immediately notice the difference across the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting region \n",
    "We talk before about interesting region localizations. Grad-cam can be also used to extract the class object out of the image. Easily, once the have the grad-cam image we can used it as mask to crop out form the input image what we want. The reader can play with the `TR` parameter to see different effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_pretrained.eval()\n",
    "\n",
    "vis = GradCam(alexnet_pretrained, device)\n",
    "\n",
    "_ = vis(inputs[0], None, postprocessing=image_net_postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def gradcam2crop(cam, original_img):\n",
    "    b, c, w, h = inputs[0].shape\n",
    "    cam = cam.numpy()\n",
    "    cam -= np.min(cam)\n",
    "    cam /= np.max(cam)\n",
    "\n",
    "    cam = cv2.resize(cam, (w, h))\n",
    "    mask = cam > TR\n",
    "\n",
    "    original_img = tensor2img(image_net_postprocessing(original_img[0].squeeze()))\n",
    "\n",
    "    crop = original_img.copy()\n",
    "    crop[mask == 0] = 0\n",
    "\n",
    "    return crop\n",
    "\n",
    "\n",
    "crop = gradcam2crop(vis.cam.cpu(), inputs[0].cpu())\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*et voil√†*! We can also change again class, and crop the interest region for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vis(inputs[0], None, target_class=231, postprocessing=image_net_postprocessing)\n",
    "\n",
    "crop = gradcam2crop(vis.cam.cpu(), inputs[0].cpu())\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
